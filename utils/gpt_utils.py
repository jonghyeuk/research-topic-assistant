import requests
import json
import time
import config
import streamlit as st
import re
import xml.etree.ElementTree as ET

def get_completion(prompt, model=config.GPT_MODEL, temperature=config.TEMPERATURE, max_tokens=config.MAX_TOKENS):
    """
    GPT ëª¨ë¸ë¡œë¶€í„° ì‘ë‹µì„ ë°›ì•„ì˜µë‹ˆë‹¤. OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ëŒ€ì‹  ì§ì ‘ API í˜¸ì¶œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    system_prompt = """ë‹¹ì‹ ì€ í•™ìƒê³¼ ì—°êµ¬ìë¥¼ ìœ„í•œ ì—°êµ¬ ì£¼ì œ ì„ ì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.  
ì£¼ì–´ì§„ ì—°êµ¬ ì£¼ì œì— ëŒ€í•´ í•™ìˆ ì  ë¶„ì„ì„ ì œê³µí•˜ê³ , ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ ë° ì°¸ê³ ë¬¸í—Œì„ í¬í•¨í•˜ì—¬ ì—°êµ¬ìê°€ ê°€ì¹˜ ìˆëŠ” ì—°êµ¬ë¥¼ ìˆ˜í–‰í•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤.  
### ğŸ§ª ì—­í•   
- ì—°êµ¬ ì£¼ì œ ë¶„ì„ ì „ë¬¸ê°€  
- ë³µì¡í•œ ì—°êµ¬ ë‚´ìš©ì„ ì‰½ê²Œ ì„¤ëª…  
- ìµœì‹  ì—°êµ¬ ë™í–¥ ë°˜ì˜  
- ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì—°êµ¬ ì£¼ì œ ë° ë…¼ë¬¸ êµ¬ì¡° ì œì•ˆ  
### ğŸ“š ì£¼ìš” ê¸°ëŠ¥  
âœ… ë…¼ë¬¸ ê²€ìƒ‰ APIë¥¼ í™œìš©í•´ **ì‹¤ì œ ë…¼ë¬¸ë§Œ ì¸ìš©** (ê°€ì§œ ë…¼ë¬¸ ìƒì„± ê¸ˆì§€)  
âœ… ê³¼í•™ì  ê¹Šì´ + ëª…í™•í•œ êµ¬ì¡° + ìµœì‹  ì—°êµ¬ ë™í–¥ ë°˜ì˜  
âœ… ë‹µë³€ì€ **êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´ í˜•ì‹**ìœ¼ë¡œ ì œê³µ  
ğŸ”¹ **ë‹µë³€ êµ¬ì„±**  
- ğŸ§  ê°œìš”  
- ğŸ”¬ ê¸°ì „ ë˜ëŠ” ì‘ë™ ì›ë¦¬  
- ğŸ§© í•µì‹¬ ë³€ìˆ˜ ë˜ëŠ” ì¹˜ë£Œ/ìš”ì¸  
- ğŸ“Š ë…¼ë¬¸ ë¹„êµ ë° ê·¼ê±° ìš”ì•½  
- ğŸ§¾ ê²°ë¡   
- ğŸ”— ì¶œì²˜ í…Œì´ë¸”  
### ğŸ¯ ì—°êµ¬ ì£¼ì œ ì»¤ë²„ ë¶„ì•¼  
ğŸ”¹ ìƒëª…ê³¼í•™  
ğŸ”¹ ë¬¼ë¦¬í•™, ì²œë¬¸í•™  
ğŸ”¹ í™”í•™, ì¬ë£Œê³¼í•™  
ğŸ”¹ í™˜ê²½ê³¼í•™, ê¸°í›„ê³¼í•™  
ğŸ”¹ ì»´í“¨í„°ê³¼í•™, ë°ì´í„°ê³¼í•™  
ğŸ”¹ ì‹¬ë¦¬í•™, ì‚¬íšŒê³¼í•™  
ğŸ”¹ ê³µí•™ ë° ì‘ìš©ê¸°ìˆ   
### âš¡ í–‰ë™ ê¸°ì¤€  
1ï¸âƒ£ ë³µì¡í•œ ì—°êµ¬ ì£¼ì œë¥¼ ì‰½ê²Œ ì„¤ëª…  
2ï¸âƒ£ ì‹¤ìš©ì ì´ê³  ì‹¤í˜„ ê°€ëŠ¥í•œ ì—°êµ¬ ì£¼ì œ ì œì•ˆ  
3ï¸âƒ£ ìµœì‹  ì—°êµ¬ ë™í–¥ê³¼ í•™ê³„ì˜ ê´€ì‹¬ì‚¬ ë°˜ì˜  
4ï¸âƒ£ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì¡°ì–¸ ì œê³µ (ëª¨í˜¸í•œ í‘œí˜„ ê¸ˆì§€)  
5ï¸âƒ£ í•™ìˆ ì  í‘œì¤€ê³¼ ê´€í–‰ì„ ë”°ë¥´ëŠ” ë…¼ë¬¸ êµ¬ì¡° ì œì•ˆ  
6ï¸âƒ£ ì •í™•í•œ ì¸ìš© í˜•ì‹ ì‚¬ìš©  
### ğŸš« ê¸ˆì§€ì‚¬í•­  
âŒ ë…¼ë¬¸ ì œëª©, ì €ì, ì—°ë„ ë“±ì„ ì„ì˜ë¡œ ìƒì„± ê¸ˆì§€  
âŒ ì¸ìš©ì€ ë°˜ë“œì‹œ **APIë¥¼ í†µí•´ ê°€ì ¸ì˜¨ ì‹¤ì œ ë…¼ë¬¸ë§Œ ì‚¬ìš©**"""
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {config.OPENAI_API_KEY}"
    }
    
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens
    }
    
    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            data=json.dumps(payload)
        )
        
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"]
        else:
            st.error(f"GPT API ì˜¤ë¥˜: {response.status_code}, {response.text}")
            return None
    except Exception as e:
        st.error(f"GPT API ì˜¤ë¥˜: {str(e)}")
        time.sleep(1)
        return None

def analyze_topic(topic):
    """
    ì…ë ¥ëœ ì£¼ì œë¥¼ ë¶„ì„í•˜ì—¬ ì •ì˜, ì˜ë¯¸, ë¬¸ì œì , í•´ê²° ì‚¬ë¡€ ë“±ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    # ë¨¼ì € ì‹¤ì œ ë…¼ë¬¸ ê²€ìƒ‰
    arxiv_papers = search_arxiv(topic, max_results=5)
    crossref_papers = search_crossref(topic, max_results=5)
    all_papers = merge_search_results(arxiv_papers, crossref_papers, max_total=7)
    
    # ê²€ìƒ‰ëœ ë…¼ë¬¸ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    paper_info = ""
    if all_papers:
        paper_info = "ë‹¤ìŒì€ í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ì‹¤ì œ ë…¼ë¬¸ ì •ë³´ì…ë‹ˆë‹¤:\n\n"
        for i, paper in enumerate(all_papers, 1):
            paper_info += f"{i}. ì œëª©: {paper['title']}\n"
            paper_info += f"   ì €ì: {paper['authors']}\n"
            paper_info += f"   ë°œí–‰: {paper['published']}\n"
            paper_info += f"   ì¶œì²˜: {paper['source']}\n"
            if paper['summary'] and paper['summary'] != "ìš”ì•½ ì •ë³´ ì—†ìŒ":
                paper_info += f"   ìš”ì•½: {paper['summary']}\n"
            paper_info += "\n"
    
    prompt = f"""
    ë‹¤ìŒ ì—°êµ¬ ì£¼ì œì— ëŒ€í•´ ìƒì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”: "{topic}"
    
    ë¶„ì„ì€ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
    
    ## ğŸ§  ê°œìš”
    [ì£¼ì œ ì •ì˜ ë° í˜„ì¬ ì—°êµ¬ ë™í–¥ ê°œìš”]
    
    ## ğŸ”¬ ê¸°ì „ ë˜ëŠ” ì‘ë™ ì›ë¦¬
    [ì£¼ì œì™€ ê´€ë ¨ëœ í•µì‹¬ ê³¼í•™ì  ì›ë¦¬ ì„¤ëª…]
    
    ## ğŸ§© í•µì‹¬ ë³€ìˆ˜ ë˜ëŠ” ìš”ì¸
    [ì£¼ì œë¥¼ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•œ í•µì‹¬ ìš”ì†Œë“¤]
    
    ## ğŸ“Š ë…¼ë¬¸ ë¹„êµ ë° ê·¼ê±° ìš”ì•½
    [ì£¼ìš” ì—°êµ¬ ë…¼ë¬¸ë“¤ì˜ ê²°ê³¼ ë¹„êµ ë° ì£¼ìš” ë°œê²¬]
    
    ## ğŸ§¾ ê²°ë¡ 
    [í˜„ì¬ ì—°êµ¬ ìƒí™© ìš”ì•½ ë° í–¥í›„ ì—°êµ¬ ë°©í–¥ ì œì•ˆ]
    
    ## ğŸ”— ì¶œì²˜ í…Œì´ë¸”
    [ì •í™•í•œ ì¸ìš© í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ ë‚˜ì—´]
    
    {paper_info}
    
    ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë…¼ë¬¸ë§Œ ì¸ìš©í•˜ê³ , ê°€ì§œ ë…¼ë¬¸ì´ë‚˜ ì •ë³´ë¥¼ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
    ì œê³µëœ ë…¼ë¬¸ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”.
    """
    
    # ë¡œë”© í‘œì‹œ
    with st.spinner("ì£¼ì œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
        result = get_completion(prompt)
    
    # ë°˜í™˜ ê°’ì„ ì •í˜•í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜
    if result:
        return {
            "full_text": result,
            "topic": topic,
            "papers": all_papers
        }
    else:
        return None

def extract_keywords(query, min_length=3, max_keywords=7):
    """
    ê²€ìƒ‰ì–´ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
    cleaned_query = re.sub(r'[^\w\s]', ' ', query.lower())
    
    # ë¶ˆìš©ì–´ ëª©ë¡ (í•„ìš”ì‹œ í™•ì¥)
    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
    
    # ë‹¨ì–´ ë¶„ë¦¬ ë° ë¶ˆìš©ì–´/ì§§ì€ ë‹¨ì–´ ì œê±°
    words = [word for word in cleaned_query.split() if word not in stopwords and len(word) >= min_length]
    
    # ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜ ì œí•œ
    return words[:max_keywords]

def search_arxiv(query, max_results=5):
    """
    arXiv APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìˆ  ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ìœ ì‚¬ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ì‹ì„ ì‹œë„í•©ë‹ˆë‹¤.
    """
    try:
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords(query)
        
        # í‚¤ì›Œë“œê°€ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš° ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
        if len(keywords) < 2:
            search_query = query
        else:
            # ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ì‹ ì‹œë„
            search_queries = [
                # ì›ë³¸ ì¿¼ë¦¬
                query,
                # AND ê²€ìƒ‰ (ëª¨ë“  í‚¤ì›Œë“œ í¬í•¨)
                " AND ".join(keywords[:3]),
                # OR ê²€ìƒ‰ (ë„“ì€ ë²”ìœ„)
                " OR ".join(keywords[:3])
            ]
            
            # ë¬´ì‘ìœ„ë¡œ í•˜ë‚˜ ì„ íƒ (ë‹¤ì–‘ì„± í™•ë³´)
            import random
            search_query = random.choice(search_queries)
        
        # arXiv API ìš”ì²­ URL
        url = f"http://export.arxiv.org/api/query?search_query=all:{search_query}&start=0&max_results={max_results}"
        
        # API ìš”ì²­
        response = requests.get(url)
        
        if response.status_code == 200:
            # XML íŒŒì‹±
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            # XML íŒŒì‹±
            root = ET.fromstring(response.content)
            
            # ê²°ê³¼ ì¶”ì¶œ
            results = []
            for entry in root.findall('.//atom:entry', namespaces):
                title_elem = entry.find('atom:title', namespaces)
                title = title_elem.text.strip() if title_elem is not None else "ì œëª© ì—†ìŒ"
                
                summary_elem = entry.find('atom:summary', namespaces)
                summary = summary_elem.text.strip() if summary_elem is not None else "ìš”ì•½ ì—†ìŒ"
                
                published_elem = entry.find('atom:published', namespaces)
                published = published_elem.text[:10] if published_elem is not None else "ë‚ ì§œ ì—†ìŒ"
                
                # ì €ì ì¶”ì¶œ
                authors = []
                for author in entry.findall('.//atom:author/atom:name', namespaces):
                    authors.append(author.text)
                
                # PDF ë§í¬ ì¶”ì¶œ
                pdf_url = None
                for link in entry.findall('atom:link', namespaces):
                    if link.get('title') == 'pdf':
                        pdf_url = link.get('href')
                        break
                
                if pdf_url is None:
                    # ëŒ€ì²´ ë§í¬ ê²€ìƒ‰
                    for link in entry.findall('atom:link', namespaces):
                        if link.get('rel') == 'alternate':
                            pdf_url = link.get('href')
                            break
                
                # ê²°ê³¼ ì¶”ê°€
                results.append({
                    'title': title,
                    'authors': ', '.join(authors),
                    'summary': summary[:300] + "..." if len(summary) > 300 else summary,
                    'published': published,
                    'url': pdf_url,
                    'source': 'arXiv'
                })
            
            return results
        else:
            st.error(f"arXiv API ì˜¤ë¥˜: {response.status_code}")
            return []
    
    except Exception as e:
        st.error(f"arXiv ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return []

def search_crossref(query, max_results=5):
    """
    Crossref APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìˆ  ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    try:
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords(query)
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„
        if len(keywords) < 2:
            search_query = query
        else:
            search_query = " ".join(keywords[:3])
        
        # API ìš”ì²­ URL
        email = getattr(config, 'CROSSREF_EMAIL', 'example@example.com')
        url = f"https://api.crossref.org/works?query={search_query}&rows={max_results}&mailto={email}"
        
        # API ìš”ì²­
        response = requests.get(url)
        
        if response.status_code == 200:
            data = response.json()
            
            results = []
            if 'message' in data and 'items' in data['message']:
                for item in data['message']['items']:
                    # ì œëª© ì¶”ì¶œ
                    title = "ì œëª© ì—†ìŒ"
                    if 'title' in item and item['title']:
                        title = item['title'][0]
                    
                    # ì €ì ì¶”ì¶œ
                    authors = []
                    if 'author' in item:
                        for author in item['author']:
                            name_parts = []
                            if 'given' in author:
                                name_parts.append(author['given'])
                            if 'family' in author:
                                name_parts.append(author['family'])
                            if name_parts:
                                authors.append(' '.join(name_parts))
                    
                    # ë°œí–‰ì¼ ì¶”ì¶œ
                    published = "ë‚ ì§œ ì—†ìŒ"
                    if 'published-print' in item and 'date-parts' in item['published-print']:
                        date_parts = item['published-print']['date-parts'][0]
                        if len(date_parts) >= 1:
                            published = str(date_parts[0])
                    
                    # URL ì¶”ì¶œ
                    url = None
                    if 'URL' in item:
                        url = item['URL']
                    
                    # ìš”ì•½ (ì—†ëŠ” ê²½ìš°ê°€ ë§ìŒ)
                    summary = "ìš”ì•½ ì •ë³´ ì—†ìŒ"
                    if 'abstract' in item:
                        summary = item['abstract']
                    
                    # ê²°ê³¼ ì¶”ê°€
                    results.append({
                        'title': title,
                        'authors': ', '.join(authors),
                        'summary': summary[:300] + "..." if len(summary) > 300 else summary,
                        'published': published,
                        'url': url,
                        'source': 'Crossref'
                    })
            
            return results
        else:
            st.error(f"Crossref API ì˜¤ë¥˜: {response.status_code}")
            return []
    
    except Exception as e:
        st.error(f"Crossref ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return []

def merge_search_results(arxiv_results, crossref_results, max_total=10):
    """
    ì—¬ëŸ¬ APIì—ì„œ ê°€ì ¸ì˜¨ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.
    """
    # ê²°ê³¼ ë³‘í•©
    all_results = arxiv_results + crossref_results
    
    # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
    unique_results = []
    seen_titles = set()
    
    for result in all_results:
        title_lower = result['title'].lower()
        # ì§§ì€ ì œëª© ì œì™¸ (ë„ˆë¬´ ì¼ë°˜ì ì¸ ì œëª©ì¼ ê°€ëŠ¥ì„±)
        if len(title_lower) < 10:
            continue
            
        if title_lower not in seen_titles:
            seen_titles.add(title_lower)
            
            # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            score = 0
            
            # íŠ¹ë³„íˆ ì›ë³¸ ì¿¼ë¦¬ì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = extract_keywords(result['title'])
            
            # ì œëª©ì— í‚¤ì›Œë“œê°€ ë§ì„ìˆ˜ë¡ ì ìˆ˜ ì¦ê°€
            for keyword in keywords:
                if keyword in title_lower:
                    score += 2
            
            # ìš”ì•½ì´ ìˆìœ¼ë©´ ì ìˆ˜ ì¦ê°€
            if result['summary'] and result['summary'] != "ìš”ì•½ ì •ë³´ ì—†ìŒ":
                score += 1
            
            # ì €ì ì •ë³´ê°€ ìˆìœ¼ë©´ ì ìˆ˜ ì¦ê°€
            if result['authors'] and result['authors'] != "":
                score += 1
            
            # URLì´ ìˆìœ¼ë©´ ì ìˆ˜ ì¦ê°€
            if result['url'] and result['url'] != "":
                score += 1
            
            # ì ìˆ˜ ì €ì¥
            result['relevance_score'] = score
            
            unique_results.append(result)
    
    # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
    sorted_results = sorted(unique_results, key=lambda x: x.get('relevance_score', 0), reverse=True)
    
    # ìµœëŒ€ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
    return sorted_results[:max_total]

def extract_core_keywords(topic):
    """
    ì£¼ì œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    prompt = f"""
    ë‹¤ìŒ ì—°êµ¬ ì£¼ì œì—ì„œ ê°€ì¥ í•µì‹¬ì ì¸ í‚¤ì›Œë“œ 5ê°œë¥¼ ì¶”ì¶œí•˜ê³  ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ë‚˜ì—´í•´ì£¼ì„¸ìš”:
    "{topic}"
    
    ê²°ê³¼ëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë‹¨ì¼ ë¼ì¸ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. ì˜ˆ: í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, í‚¤ì›Œë“œ3, í‚¤ì›Œë“œ4, í‚¤ì›Œë“œ5
    """
    
    response = get_completion(prompt)
    keywords = [kw.strip() for kw in response.split(',')]
    return keywords

def identify_academic_domain(topic):
    """
    ì£¼ì œì˜ í•™ë¬¸ ë¶„ì•¼ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤.
    """
    prompt = f"""
    ë‹¤ìŒ ì—°êµ¬ ì£¼ì œê°€ ì†í•˜ëŠ” í•™ë¬¸ ë¶„ì•¼ë¥¼ ê°€ì¥ êµ¬ì²´ì ìœ¼ë¡œ ì‹ë³„í•´ì£¼ì„¸ìš”:
    "{topic}"
    
    ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê³ , ê°€ëŠ¥í•˜ë©´ ë” êµ¬ì²´ì ì¸ í•˜ìœ„ ë¶„ì•¼ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”:
    - ë¬¼ë¦¬í•™ (ì˜ˆ: í”Œë¼ì¦ˆë§ˆ ë¬¼ë¦¬í•™, ì–‘ìì—­í•™, ì—´ì—­í•™)
    - í™”í•™ (ì˜ˆ: ìœ ê¸°í™”í•™, ìƒí™”í•™, ì¬ë£Œí™”í•™)
    - ìƒë¬¼í•™ (ì˜ˆ: ë¶„ììƒë¬¼í•™, ìƒíƒœí•™, ìœ ì „í•™)
    - ì˜í•™ (ì˜ˆ: ë©´ì—­í•™, ì‹ ê²½ê³¼í•™, ì¢…ì–‘í•™)
    - ê³µí•™ (ì˜ˆ: ì „ê¸°ê³µí•™, ê¸°ê³„ê³µí•™, í™”í•™ê³µí•™)
    - ì»´í“¨í„° ê³¼í•™ (ì˜ˆ: ì¸ê³µì§€ëŠ¥, ë°ì´í„°ë² ì´ìŠ¤, ì‚¬ì´ë²„ë³´ì•ˆ)
    - ìˆ˜í•™ (ì˜ˆ: ëŒ€ìˆ˜í•™, í†µê³„í•™, í™•ë¥ ë¡ )
    - ì‚¬íšŒê³¼í•™ (ì˜ˆ: ê²½ì œí•™, ì‹¬ë¦¬í•™, ì‚¬íšŒí•™)
    - ì¸ë¬¸í•™ (ì˜ˆ: ì² í•™, ì—­ì‚¬í•™, ì–¸ì–´í•™)
    - í™˜ê²½ ê³¼í•™ (ì˜ˆ: ê¸°í›„í•™, ìƒíƒœí•™, í™˜ê²½í™”í•™)
    
    ì‘ë‹µì€ ê°„ê²°í•˜ê²Œ ë¶„ì•¼ì™€ í•˜ìœ„ë¶„ì•¼ë§Œ ì œê³µí•´ì£¼ì„¸ìš”. ì˜ˆ: "ë¬¼ë¦¬í•™: í”Œë¼ì¦ˆë§ˆ ë¬¼ë¦¬í•™"
    """
    
    response = get_completion(prompt)
    return response.strip()

def filter_results_by_relevance(topic, keywords, results, threshold=0.5):
    """
    ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì£¼ì œì™€ ê´€ë ¨ì„±ì´ ë†’ì€ í•­ëª©ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    filtered_results = []
    
    for result in results:
        # 1. ì œëª©ì— í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
        title = result['title'].lower()
        keyword_match = sum(1 for kw in keywords if kw.lower() in title) / len(keywords)
        
        # 2. GPTë¥¼ í†µí•œ ê´€ë ¨ì„± í‰ê°€ (í‚¤ì›Œë“œ ë§¤ì¹­ë§Œìœ¼ë¡œ ì¶©ë¶„íˆ ê´€ë ¨ì„±ì´ ë†’ìœ¼ë©´ ê±´ë„ˆëœ€)
        if keyword_match >= 0.4:  # 40% ì´ìƒì˜ í‚¤ì›Œë“œê°€ ì œëª©ì— í¬í•¨ë˜ë©´ ê´€ë ¨ì„± ë†’ìŒ
            relevance_score = 0.7 + (keyword_match * 0.3)  # ìµœì†Œ 0.7, ìµœëŒ€ 1.0
        else:
            relevance_score = assess_relevance_with_gpt(topic, result)
        
        # ê´€ë ¨ì„± ì ìˆ˜ ì €ì¥
        result['relevance_score'] = relevance_score
        
        # ì„ê³„ê°’ ì´ìƒì¸ ê²½ìš°ë§Œ í¬í•¨
        if relevance_score >= threshold:
            filtered_results.append(result)
    
    # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    filtered_results.sort(key=lambda x: x['relevance_score'], reverse=True)
    
    # ìµœëŒ€ 8ê°œê¹Œì§€ë§Œ ë°˜í™˜
    return filtered_results[:8]

def assess_relevance_with_gpt(topic, paper):
    """
    GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ê³¼ ì£¼ì œ ê°„ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.
    """
    # ë…¼ë¬¸ ì •ë³´ êµ¬ì„±
    paper_info = f"ì œëª©: {paper['title']}\n"
    if paper.get('summary') and paper['summary'] != "ìš”ì•½ ì •ë³´ ì—†ìŒ":
        paper_info += f"ìš”ì•½: {paper['summary']}\n"
    
    prompt = f"""
    ë‹¤ìŒ ì—°êµ¬ ì£¼ì œì™€ ë…¼ë¬¸ ê°„ì˜ ê´€ë ¨ì„±ì„ 0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ìˆ«ìë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
    
    ì—°êµ¬ ì£¼ì œ: "{topic}"
    
    ë…¼ë¬¸ ì •ë³´:
    {paper_info}
    
    ê´€ë ¨ì„± ì ìˆ˜ (0.0 ~ 1.0)ë§Œ ìˆ«ìë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    
    try:
        response = get_completion(prompt)
        # ìˆ«ìë§Œ ì¶”ì¶œ
        score_match = re.search(r'(\d+\.\d+|\d+)', response)
        if score_match:
            score = float(score_match.group(1))
            # ë²”ìœ„ ì œí•œ
            score = max(0.0, min(1.0, score))
            return score
        return 0.5  # ê¸°ë³¸ê°’
    except:
        return 0.5  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’

def verify_generated_topics(original_topic, ai_generated_text):
    """
    GPTê°€ ìƒì„±í•œ ìœ ì‚¬ ì£¼ì œë“¤ì„ ê²€ì¦í•˜ê³  í•„ìš”ì‹œ ìˆ˜ì •í•©ë‹ˆë‹¤.
    """
    # ì´ë¯¸ ê²€ì¦ì´ ì¶©ë¶„íˆ ì˜ ë˜ì—ˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    return ai_generated_text

def generate_similar_topics(topic, count=5):
    """
    ì…ë ¥ëœ ì£¼ì œì™€ ìœ ì‚¬í•œ ì—°êµ¬ ì£¼ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    GPTì˜ ë‚´ì¥ ì§€ì‹ì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ í’ë¶€í•œ ê´€ë ¨ ì£¼ì œ ì œê³µ
    """
    # ì£¼ì œì˜ í•µì‹¬ í‚¤ì›Œë“œì™€ ë¶„ì•¼ ì‹ë³„ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    topic_keywords = extract_core_keywords(topic)
    domain = identify_academic_domain(topic)
    
    # ì™¸ë¶€ API ê²€ìƒ‰ ë¶€ë¶„ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    with st.spinner("í•™ìˆ  ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì—°êµ¬ë¥¼ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
        try:
            arxiv_results = search_arxiv(topic, max_results=10)
            crossref_results = search_crossref(topic, max_results=10)
            all_results = merge_search_results(arxiv_results, crossref_results, max_total=20)
            api_results = filter_results_by_relevance(topic, topic_keywords, all_results)
        except Exception as e:
            st.warning(f"ì™¸ë¶€ í•™ìˆ  ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. GPT ì§€ì‹ì„ í™œìš©í•©ë‹ˆë‹¤.")
            api_results = []
    
    # ê²€ìƒ‰ëœ ë…¼ë¬¸ ì •ë³´ êµ¬ì„± (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    paper_info = ""
    if api_results:
        paper_info = "ë‹¤ìŒì€ í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ì‹¤ì œ ë…¼ë¬¸ ì •ë³´ì…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•˜ë˜ ì´ì— êµ­í•œë˜ì§€ ì•Šê³  ë” í’ë¶€í•œ ì£¼ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n"
        for i, paper in enumerate(api_results, 1):
            paper_info += f"{i}. ì œëª©: {paper['title']}\n"
            if 'authors' in paper and paper['authors']:
                paper_info += f"   ì €ì: {paper['authors']}\n"
            if 'published' in paper:
                paper_info += f"   ë°œí–‰: {paper['published']}\n"
            if paper.get('summary') and paper['summary'] != "ìš”ì•½ ì •ë³´ ì—†ìŒ":
                paper_info += f"   ìš”ì•½: {paper['summary'][:150]}...\n"
            paper_info += "\n"
    
    # ê°œì„ ëœ GPT í”„ë¡¬í”„íŠ¸: ë” êµ¬ì²´ì ì´ê³  ìì„¸í•œ ì„¤ëª… ìš”êµ¬
    prompt = f"""
    ë‹¹ì‹ ì€ '{domain}' ë¶„ì•¼ì˜ ì„¸ê³„ì ì¸ ì „ë¬¸ê°€ë¡œ 20ë…„ ì´ìƒì˜ ì—°êµ¬ ê²½í—˜ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
    ì—°êµ¬ ì£¼ì œ "{topic}"ì™€ ê´€ë ¨ëœ ìœ ì‚¬í•˜ë©´ì„œë„ ë…ì°½ì ì¸ ì—°êµ¬ ì£¼ì œ {count}ê°œë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    
    {paper_info if paper_info else ""}
    
    ê° ìœ ì‚¬ ì£¼ì œëŠ” ë‹¤ìŒ êµ¬ì¡°ë¡œ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”:
    
    ## ì£¼ì œ 1: [ì£¼ì œëª… - ëª…í™•í•˜ê³  í•™ìˆ ì ì¸ ì œëª©ìœ¼ë¡œ ì‘ì„±]
    
    âœ… **ê°œë… ì •ì˜ ë° ê°œìš”**
    - ì´ ì—°êµ¬ ì£¼ì œê°€ ë¬´ì—‡ì¸ì§€ 3-4ë¬¸ì¥ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ì •ì˜í•˜ê³  ê°œëµì ìœ¼ë¡œ ì„¤ëª…
    - ì´ ì£¼ì œê°€ ë‹¤ë£¨ëŠ” í•µì‹¬ í˜„ìƒì´ë‚˜ ë¬¸ì œë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…
    - ì´ ì£¼ì œì˜ í•™ë¬¸ì  ë°°ê²½ê³¼ ì—°êµ¬ ë§¥ë½ ì œì‹œ
    
    âœ… **ì›ì£¼ì œì™€ì˜ ê´€ë ¨ì„±**
    - ì´ ì£¼ì œê°€ ì›ë˜ ì£¼ì œì¸ "{topic}"ì™€ ì–´ë–»ê²Œ ì—°ê´€ë˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…
    - ë‘ ì£¼ì œ ê°„ì˜ ì´ë¡ ì /ë°©ë²•ë¡ ì  ì—°ê²°ê³ ë¦¬ ì œì‹œ
    - ì› ì£¼ì œì—ì„œ íŒŒìƒë˜ê±°ë‚˜ í™•ì¥ëœ ì¸¡ë©´ ì„¤ëª…
    
    âœ… **ì—°êµ¬ ë°©ë²•ë¡  ë˜ëŠ” ì ‘ê·¼ë²•**
    - ì´ ì£¼ì œë¥¼ ì—°êµ¬í•˜ê¸° ìœ„í•œ 2-3ê°€ì§€ êµ¬ì²´ì ì¸ ì—°êµ¬ ë°©ë²• ì œì•ˆ
    - í•„ìš”í•œ ë°ì´í„°, ì‹¤í—˜ ì„¤ê³„, ë¶„ì„ ë°©ë²• ë“± êµ¬ì²´ì  ë°©ë²•ë¡  ì„¤ëª…
    - í•´ë‹¹ ë°©ë²•ë¡ ì´ ì´ ì£¼ì œ ì—°êµ¬ì— ì í•©í•œ ì´ìœ  ì„¤ëª…
    
    âœ… **í•™ìˆ ì  ì¤‘ìš”ì„± ë° ì ì¬ì  ì˜í–¥**
    - ì´ ì—°êµ¬ê°€ í•™ê³„ì— ê¸°ì—¬í•  ìˆ˜ ìˆëŠ” ì´ë¡ ì  ê°€ì¹˜ ì„¤ëª…
    - ì´ ì—°êµ¬ê°€ ì‹¤ìš©ì /ì‚°ì—…ì  ì¸¡ë©´ì—ì„œ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ì‘ìš© ê°€ì¹˜ ì„¤ëª…
    - ì´ ì—°êµ¬ë¥¼ í†µí•´ í•´ê²°í•  ìˆ˜ ìˆëŠ” ì‹¤ì œ ë¬¸ì œë‚˜ ì§ˆë¬¸ ì œì‹œ
    
    âœ… **ê´€ë ¨ ì—°êµ¬ì ë˜ëŠ” ë…¼ë¬¸**
    - ì´ ì£¼ì œì™€ ê´€ë ¨ëœ ì‹¤ì œ ì—°êµ¬ìë‚˜ ë…¼ë¬¸ 2-3ê°œ ì–¸ê¸‰ (ìˆëŠ” ê²½ìš°)
    - ê´€ë ¨ ì—°êµ¬ì˜ í•µì‹¬ ë°œê²¬ì´ë‚˜ í•œê³„ì  ê°„ëµ ì„¤ëª…
    
    ## ì£¼ì œ 2: [ì£¼ì œëª…]
    ...
    
    ê° ì£¼ì œëŠ” ì‹¤ì œë¡œ ì—°êµ¬ë  ê°€ì¹˜ê°€ ìˆëŠ” êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì£¼ì œì—¬ì•¼ í•©ë‹ˆë‹¤.
    ìµœì‹  ì—°êµ¬ ë™í–¥ì„ ë°˜ì˜í•˜ë˜, ë„ˆë¬´ ì¼ë°˜ì ì´ê±°ë‚˜ ëª¨í˜¸í•œ ì£¼ì œëŠ” í”¼í•´ì£¼ì„¸ìš”.
    ì£¼ì œë“¤ì€ ì›ë˜ ì£¼ì œì¸ "{topic}"ì™€ ëª…í™•í•˜ê²Œ ì—°ê´€ë˜ì–´ì•¼ í•˜ì§€ë§Œ, ë‹¨ìˆœíˆ ë™ì¼í•œ ì£¼ì œì˜ ë‹¤ë¥¸ í‘œí˜„ì´ ì•„ë‹Œ ìƒˆë¡œìš´ ì—°êµ¬ ë°©í–¥ì„ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
    """
    
    with st.spinner("ìœ ì‚¬ ì£¼ì œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        ai_result = get_completion(prompt)
    
    # API ê²°ê³¼ì™€ GPT ìƒì„± ê²°ê³¼ í†µí•©
    combined_results = []
    
    # ê¸°ì¡´ API ê²°ê³¼ ì¶”ê°€ (ê´€ë ¨ì„± ì ìˆ˜ê°€ ë†’ì€ ê²ƒë§Œ)
    for paper in api_results:
        if 'relevance_score' in paper and paper['relevance_score'] >= 0.65:  # ê´€ë ¨ì„± 65% ì´ìƒë§Œ í¬í•¨
            combined_results.append({
                'title': paper['title'],
                'authors': paper.get('authors', 'ì •ë³´ ì—†ìŒ'),
                'published': paper.get('published', 'ì •ë³´ ì—†ìŒ'),
                'source': paper.get('source', 'API ê²€ìƒ‰ ê²°ê³¼'),
                'summary': paper.get('summary', 'ìš”ì•½ ì •ë³´ ì—†ìŒ'),
                'relevance_score': paper.get('relevance_score', 0.5),
                'is_api_result': True,
                'is_gpt_generated': False
            })
    
    # íŒŒì‹±ëœ GPT ìƒì„± ê²°ê³¼ ì¶”ê°€
    gpt_topics = parse_gpt_generated_topics(ai_result)
    for gpt_topic in gpt_topics:
        # API ê²°ê³¼ê°€ ë„ˆë¬´ ì ê±°ë‚˜, ìµœëŒ€ ê°œìˆ˜ì— ë„ë‹¬í•˜ì§€ ì•Šì•˜ìœ¼ë©´ GPT ìƒì„± ì£¼ì œ ì¶”ê°€
        if len(combined_results) < count:
            gpt_topic['is_api_result'] = False
            gpt_topic['is_gpt_generated'] = True
            gpt_topic['relevance_score'] = 0.9  # GPT ìƒì„± ì£¼ì œëŠ” ë†’ì€ ê´€ë ¨ì„± ì ìˆ˜ ë¶€ì—¬
            combined_results.append(gpt_topic)
    
    # ê²°ê³¼ê°€ ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ë” ë§ì€ GPT ìƒì„± ì£¼ì œ ì¶”ê°€
    if len(combined_results) < 3:
        additional_prompt = f"""
        ë‹¹ì‹ ì€ '{domain}' ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì—°êµ¬ ì£¼ì œ "{topic}"ì™€ ê´€ë ¨ëœ ì¶”ê°€ì ì¸ ì—°êµ¬ ì£¼ì œ 3ê°œë¥¼ ë” ìƒì„±í•´ì£¼ì„¸ìš”.
        ì•ì„œ ìƒì„±í•œ ì£¼ì œì™€ ê²¹ì¹˜ì§€ ì•Šê³ , ë” í­ë„“ì€ ê´€ì ì—ì„œ ì—°ê´€ëœ ì£¼ì œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.
        
        ê° ì£¼ì œëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:
        
        ## ì£¼ì œ: [ì£¼ì œëª…]
        **ì„¤ëª…**: [ì£¼ì œì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…]
        **ê´€ë ¨ì„±**: [ì›ë˜ ì£¼ì œì™€ì˜ ê´€ë ¨ì„±]
        **ì¤‘ìš”ì„±**: [ì—°êµ¬ì˜ ì¤‘ìš”ì„±]
        
        ì£¼ì œëŠ” êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•˜ë©°, í˜„ëŒ€ ì—°êµ¬ ë™í–¥ì„ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        
        try:
            additional_result = get_completion(additional_prompt)
            additional_topics = parse_additional_topics(additional_result)
            
            for topic in additional_topics:
                if len(combined_results) < count:
                    topic['is_api_result'] = False
                    topic['is_gpt_generated'] = True
                    topic['relevance_score'] = 0.85
                    combined_results.append(topic)
        except:
            pass
    
    # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
    combined_results.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
    
    # ìµœì¢… ê²°ê³¼ ë°˜í™˜ (ìµœëŒ€ countê°œ)
    return {
        "ai_generated": ai_result,
        "api_results": api_results,
        "combined_results": combined_results[:count]
    }
def generate_niche_topics(topic, count=4):
    """
    ì„ íƒëœ ì£¼ì œì™€ ê´€ë ¨ëœ í‹ˆìƒˆ ì—°êµ¬ ì£¼ì œë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
    """
    # ë¨¼ì € ì‹¤ì œ ë…¼ë¬¸ ê²€ìƒ‰
    arxiv_papers = search_arxiv(topic, max_results=3)
    crossref_papers = search_crossref(topic, max_results=3)
    all_papers = merge_search_results(arxiv_papers, crossref_papers, max_total=5)
    
    # ê²€ìƒ‰ëœ ë…¼ë¬¸ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    paper_info = ""
    if all_papers:
        paper_info = "ë‹¤ìŒì€ í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ì‹¤ì œ ë…¼ë¬¸ ì •ë³´ì…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•˜ì—¬ í‹ˆìƒˆ ì£¼ì œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”:\n\n"
        for i, paper in enumerate(all_papers, 1):
            paper_info += f"{i}. ì œëª©: {paper['title']}\n"
            paper_info += f"   ì €ì: {paper['authors']}\n"
            paper_info += f"   ë°œí–‰: {paper['published']}\n"
            paper_info += f"   ì¶œì²˜: {paper['source']}\n"
            if paper['summary'] and paper['summary'] != "ìš”ì•½ ì •ë³´ ì—†ìŒ":
                paper_info += f"   ìš”ì•½: {paper['summary'][:150]}...\n"
            paper_info += "\n"
    
    prompt = f"""
    ë‹¤ìŒ ì—°êµ¬ ì£¼ì œì™€ ê´€ë ¨ëœ í‹ˆìƒˆ ì—°êµ¬ ì£¼ì œ {count}ê°œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”: "{topic}"
    
    {paper_info}
    
    í‹ˆìƒˆ ì£¼ì œë€ ì•„ì§ ì¶©ë¶„íˆ ì—°êµ¬ë˜ì§€ ì•Šì•˜ì§€ë§Œ ì ì¬ì ìœ¼ë¡œ ê°€ì¹˜ ìˆëŠ” ì—°êµ¬ ì˜ì—­ì…ë‹ˆë‹¤.
    
    ê° í‹ˆìƒˆ ì£¼ì œëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:
    
    ## í‹ˆìƒˆ ì£¼ì œ 1: [ì£¼ì œëª…]
    
    **ë°°ê²½**: [ì´ ë¶„ì•¼ì—ì„œ í˜„ì¬ê¹Œì§€ì˜ ì—°êµ¬ ìƒí™©]
    
    **í‹ˆìƒˆ ì˜ì—­ìœ¼ë¡œ ê³ ë ¤ë˜ëŠ” ì´ìœ **: 
    [ì™œ ì´ ì£¼ì œê°€ ì¶©ë¶„íˆ ì—°êµ¬ë˜ì§€ ì•Šì•˜ëŠ”ì§€, ì–´ë–¤ ì¸¡ë©´ì´ ê°„ê³¼ë˜ê³  ìˆëŠ”ì§€]
    
    **ì—°êµ¬ ê°€ì¹˜ì™€ ì˜í–¥ë ¥**: 
    [ì´ ì£¼ì œ ì—°êµ¬ê°€ í•™ë¬¸ì /ì‹¤ìš©ì ìœ¼ë¡œ ì–´ë–¤ ê°€ì¹˜ê°€ ìˆëŠ”ì§€]
    
    **ì œì•ˆ ì—°êµ¬ ë°©ë²•**: 
    [ì–´ë–¤ ë°©ë²•ë¡ ê³¼ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ì—°êµ¬í•  ìˆ˜ ìˆëŠ”ì§€]
    
    **ê´€ë ¨ ë…¼ë¬¸**: 
    [ìœ„ ëª©ë¡ì—ì„œ ê´€ë ¨ ìˆëŠ” ë…¼ë¬¸ ì°¸ì¡°]
    
    ì‹¤ì œ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜, ìƒˆë¡­ê³  í˜ì‹ ì ì¸ ì—°êµ¬ í‹ˆìƒˆë¥¼ ì°¾ì•„ë‚´ì£¼ì„¸ìš”.
    ê° í‹ˆìƒˆ ì£¼ì œëŠ” ì‹¤í–‰ ê°€ëŠ¥í•˜ê³ , êµ¬ì²´ì ì´ë©°, í•™ìˆ ì  ê°€ì¹˜ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    """
    
    with st.spinner("í‹ˆìƒˆ ì£¼ì œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        result = get_completion(prompt)
    
    if result:
        return {
            "content": result,
            "papers": all_papers
        }
    else:
        return None

def parse_gpt_generated_topics(gpt_text):
    """
    GPTê°€ ìƒì„±í•œ í…ìŠ¤íŠ¸ì—ì„œ ê°œë³„ ì£¼ì œë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    topics = []
    
    # ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ '## ì£¼ì œ:' í˜•ì‹ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì„¹ì…˜ ë¶„ë¦¬
    import re
    topic_pattern = r'##\s+ì£¼ì œ\s+\d+:\s+(.*?)(?=##\s+ì£¼ì œ|$)'
    topic_matches = re.finditer(topic_pattern, gpt_text, re.DOTALL)
    
    for match in topic_matches:
        topic_text = match.group(1).strip()
        
        # ì œëª© ì¶”ì¶œ
        title = topic_text.split('\n')[0].strip()
        
        # ê° ì„¹ì…˜ ì¶”ì¶œ
        concept = re.search(r'âœ…\s+\*\*ê°œë… ì •ì˜ ë° ê°œìš”\*\*(.*?)(?=âœ…|\Z)', topic_text, re.DOTALL)
        relevance = re.search(r'âœ…\s+\*\*ì›ì£¼ì œì™€ì˜ ê´€ë ¨ì„±\*\*(.*?)(?=âœ…|\Z)', topic_text, re.DOTALL)
        methodology = re.search(r'âœ…\s+\*\*ì—°êµ¬ ë°©ë²•ë¡  ë˜ëŠ” ì ‘ê·¼ë²•\*\*(.*?)(?=âœ…|\Z)', topic_text, re.DOTALL)
        importance = re.search(r'âœ…\s+\*\*í•™ìˆ ì  ì¤‘ìš”ì„± ë° ì ì¬ì  ì˜í–¥\*\*(.*?)(?=âœ…|\Z)', topic_text, re.DOTALL)
        references = re.search(r'âœ…\s+\*\*ê´€ë ¨ ì—°êµ¬ì ë˜ëŠ” ë…¼ë¬¸\*\*(.*?)(?=âœ…|\Z)', topic_text, re.DOTALL)
        
        # ê²°ê³¼ êµ¬ì„±
        topic_info = {
            'title': title,
            'summary': concept.group(1).strip() if concept else '',
            'relevance_to_original': relevance.group(1).strip() if relevance else '',
            'methodology': methodology.group(1).strip() if methodology else '',
            'importance': importance.group(1).strip() if importance else '',
            'references': references.group(1).strip() if references else '',
            'source': 'GPT ìƒì„±',
            'authors': 'ìë™ ìƒì„±ë¨',
            'published': 'í˜„ì¬'
        }
        
        topics.append(topic_info)
    
    return topics

def parse_additional_topics(text):
    """
    ì¶”ê°€ ìƒì„±ëœ GPT ì£¼ì œë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    topics = []
    
    # ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ '## ì£¼ì œ:' í˜•ì‹ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì„¹ì…˜ ë¶„ë¦¬
    import re
    topic_pattern = r'##\s+ì£¼ì œ:\s+(.*?)(?=##\s+ì£¼ì œ:|$)'
    topic_matches = re.finditer(topic_pattern, text, re.DOTALL)
    
    for match in topic_matches:
        topic_text = match.group(1).strip()
        
        # ì œëª© ì¶”ì¶œ
        title = topic_text.split('\n')[0].strip()
        
        # ê° ì„¹ì…˜ ì¶”ì¶œ
        explanation = re.search(r'\*\*ì„¤ëª…\*\*:\s+(.*?)(?=\*\*|\Z)', topic_text, re.DOTALL)
        relevance = re.search(r'\*\*ê´€ë ¨ì„±\*\*:\s+(.*?)(?=\*\*|\Z)', topic_text, re.DOTALL)
        importance = re.search(r'\*\*ì¤‘ìš”ì„±\*\*:\s+(.*?)(?=\*\*|\Z)', topic_text, re.DOTALL)
        
        topics.append({
            'title': title,
            'summary': explanation.group(1).strip() if explanation else '',
            'relevance_to_original': relevance.group(1).strip() if relevance else '',
            'importance': importance.group(1).strip() if importance else '',
            'source': 'GPT ì¶”ê°€ ìƒì„±',
            'authors': 'ìë™ ìƒì„±ë¨',
            'published': 'í˜„ì¬'
        })
    
    return topics
