import requests
import json
import time
import config
import streamlit as st
import re
import xml.etree.ElementTree as ET

def get_completion(prompt, model=config.GPT_MODEL, temperature=config.TEMPERATURE, max_tokens=config.MAX_TOKENS):
    """
    GPT ëª¨ë¸ë¡œë¶€í„° ì‘ë‹µì„ ë°›ì•„ì˜µë‹ˆë‹¤. OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ëŒ€ì‹  ì§ì ‘ API í˜¸ì¶œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    system_prompt = """ë‹¹ì‹ ì€ í•™ìƒê³¼ ì—°êµ¬ìë¥¼ ìœ„í•œ ì—°êµ¬ ì£¼ì œ ì„ ì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.  
ì£¼ì–´ì§„ ì—°êµ¬ ì£¼ì œì— ëŒ€í•´ í•™ìˆ ì  ë¶„ì„ì„ ì œê³µí•˜ê³ , ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ ë° ì°¸ê³ ë¬¸í—Œì„ í¬í•¨í•˜ì—¬ ì—°êµ¬ìê°€ ê°€ì¹˜ ìˆëŠ” ì—°êµ¬ë¥¼ ìˆ˜í–‰í•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤.  
### ğŸ§ª ì—­í•   
- ì—°êµ¬ ì£¼ì œ ë¶„ì„ ì „ë¬¸ê°€  
- ë³µì¡í•œ ì—°êµ¬ ë‚´ìš©ì„ ì‰½ê²Œ ì„¤ëª…  
- ìµœì‹  ì—°êµ¬ ë™í–¥ ë°˜ì˜  
- ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì—°êµ¬ ì£¼ì œ ë° ë…¼ë¬¸ êµ¬ì¡° ì œì•ˆ  
### ğŸ“š ì£¼ìš” ê¸°ëŠ¥  
âœ… ë…¼ë¬¸ ê²€ìƒ‰ APIë¥¼ í™œìš©í•´ **ì‹¤ì œ ë…¼ë¬¸ë§Œ ì¸ìš©** (ê°€ì§œ ë…¼ë¬¸ ìƒì„± ê¸ˆì§€)  
âœ… ê³¼í•™ì  ê¹Šì´ + ëª…í™•í•œ êµ¬ì¡° + ìµœì‹  ì—°êµ¬ ë™í–¥ ë°˜ì˜  
âœ… ë‹µë³€ì€ **êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´ í˜•ì‹**ìœ¼ë¡œ ì œê³µ  
ğŸ”¹ **ë‹µë³€ êµ¬ì„±**  
- ğŸ§  ê°œìš”  
- ğŸ”¬ ê¸°ì „ ë˜ëŠ” ì‘ë™ ì›ë¦¬  
- ğŸ§© í•µì‹¬ ë³€ìˆ˜ ë˜ëŠ” ì¹˜ë£Œ/ìš”ì¸  
- ğŸ“Š ë…¼ë¬¸ ë¹„êµ ë° ê·¼ê±° ìš”ì•½  
- ğŸ§¾ ê²°ë¡   
- ğŸ”— ì¶œì²˜ í…Œì´ë¸”  
### ğŸ¯ ì—°êµ¬ ì£¼ì œ ì»¤ë²„ ë¶„ì•¼  
ğŸ”¹ ìƒëª…ê³¼í•™  
ğŸ”¹ ë¬¼ë¦¬í•™, ì²œë¬¸í•™  
ğŸ”¹ í™”í•™, ì¬ë£Œê³¼í•™  
ğŸ”¹ í™˜ê²½ê³¼í•™, ê¸°í›„ê³¼í•™  
ğŸ”¹ ì»´í“¨í„°ê³¼í•™, ë°ì´í„°ê³¼í•™  
ğŸ”¹ ì‹¬ë¦¬í•™, ì‚¬íšŒê³¼í•™  
ğŸ”¹ ê³µí•™ ë° ì‘ìš©ê¸°ìˆ   
### âš¡ í–‰ë™ ê¸°ì¤€  
1ï¸âƒ£ ë³µì¡í•œ ì—°êµ¬ ì£¼ì œë¥¼ ì‰½ê²Œ ì„¤ëª…  
2ï¸âƒ£ ì‹¤ìš©ì ì´ê³  ì‹¤í˜„ ê°€ëŠ¥í•œ ì—°êµ¬ ì£¼ì œ ì œì•ˆ  
3ï¸âƒ£ ìµœì‹  ì—°êµ¬ ë™í–¥ê³¼ í•™ê³„ì˜ ê´€ì‹¬ì‚¬ ë°˜ì˜  
4ï¸âƒ£ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì¡°ì–¸ ì œê³µ (ëª¨í˜¸í•œ í‘œí˜„ ê¸ˆì§€)  
5ï¸âƒ£ í•™ìˆ ì  í‘œì¤€ê³¼ ê´€í–‰ì„ ë”°ë¥´ëŠ” ë…¼ë¬¸ êµ¬ì¡° ì œì•ˆ  
6ï¸âƒ£ ì •í™•í•œ ì¸ìš© í˜•ì‹ ì‚¬ìš©  
### ğŸš« ê¸ˆì§€ì‚¬í•­  
âŒ ë…¼ë¬¸ ì œëª©, ì €ì, ì—°ë„ ë“±ì„ ì„ì˜ë¡œ ìƒì„± ê¸ˆì§€  
âŒ ì¸ìš©ì€ ë°˜ë“œì‹œ **APIë¥¼ í†µí•´ ê°€ì ¸ì˜¨ ì‹¤ì œ ë…¼ë¬¸ë§Œ ì‚¬ìš©**"""
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {config.OPENAI_API_KEY}"
    }
    
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens
    }
    
    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            data=json.dumps(payload)
        )
        
        if response.status_code == 200:
            return response.json()["choices"][0]["message"]["content"]
        else:
            st.error(f"GPT API ì˜¤ë¥˜: {response.status_code}, {response.text}")
            return None
    except Exception as e:
        st.error(f"GPT API ì˜¤ë¥˜: {str(e)}")
        time.sleep(1)
        return None

def analyze_topic(topic):
    """
    ì…ë ¥ëœ ì£¼ì œë¥¼ ë¶„ì„í•˜ì—¬ ì •ì˜, ì˜ë¯¸, ë¬¸ì œì , í•´ê²° ì‚¬ë¡€ ë“±ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    # ë¨¼ì € ì‹¤ì œ ë…¼ë¬¸ ê²€ìƒ‰
    arxiv_papers = search_arxiv(topic, max_results=5)
    crossref_papers = search_crossref(topic, max_results=5)
    all_papers = merge_search_results(arxiv_papers, crossref_papers, max_total=7)
    
    # ê²€ìƒ‰ëœ ë…¼ë¬¸ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    paper_info = ""
    if all_papers:
        paper_info = "ë‹¤ìŒì€ í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ì‹¤ì œ ë…¼ë¬¸ ì •ë³´ì…ë‹ˆë‹¤:\n\n"
        for i, paper in enumerate(all_papers, 1):
            paper_info += f"{i}. ì œëª©: {paper['title']}\n"
            paper_info += f"   ì €ì: {paper['authors']}\n"
            paper_info += f"   ë°œí–‰: {paper['published']}\n"
            paper_info += f"   ì¶œì²˜: {paper['source']}\n"
            if paper['summary'] and paper['summary'] != "ìš”ì•½ ì •ë³´ ì—†ìŒ":
                paper_info += f"   ìš”ì•½: {paper['summary']}\n"
            paper_info += "\n"
    
    prompt = f"""
    ë‹¤ìŒ ì—°êµ¬ ì£¼ì œì— ëŒ€í•´ ìƒì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”: "{topic}"
    
    ë¶„ì„ì€ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
    
    ## ğŸ§  ê°œìš”
    [ì£¼ì œ ì •ì˜ ë° í˜„ì¬ ì—°êµ¬ ë™í–¥ ê°œìš”]
    
    ## ğŸ”¬ ê¸°ì „ ë˜ëŠ” ì‘ë™ ì›ë¦¬
    [ì£¼ì œì™€ ê´€ë ¨ëœ í•µì‹¬ ê³¼í•™ì  ì›ë¦¬ ì„¤ëª…]
    
    ## ğŸ§© í•µì‹¬ ë³€ìˆ˜ ë˜ëŠ” ìš”ì¸
    [ì£¼ì œë¥¼ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•œ í•µì‹¬ ìš”ì†Œë“¤]
    
    ## ğŸ“Š ë…¼ë¬¸ ë¹„êµ ë° ê·¼ê±° ìš”ì•½
    [ì£¼ìš” ì—°êµ¬ ë…¼ë¬¸ë“¤ì˜ ê²°ê³¼ ë¹„êµ ë° ì£¼ìš” ë°œê²¬]
    
    ## ğŸ§¾ ê²°ë¡ 
    [í˜„ì¬ ì—°êµ¬ ìƒí™© ìš”ì•½ ë° í–¥í›„ ì—°êµ¬ ë°©í–¥ ì œì•ˆ]
    
    ## ğŸ”— ì¶œì²˜ í…Œì´ë¸”
    [ì •í™•í•œ ì¸ìš© í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ ë‚˜ì—´]
    
    {paper_info}
    
    ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë…¼ë¬¸ë§Œ ì¸ìš©í•˜ê³ , ê°€ì§œ ë…¼ë¬¸ì´ë‚˜ ì •ë³´ë¥¼ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
    ì œê³µëœ ë…¼ë¬¸ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”.
    """
    
    # ë¡œë”© í‘œì‹œ
    with st.spinner("ì£¼ì œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
        result = get_completion(prompt)
    
    # ë°˜í™˜ ê°’ì„ ì •í˜•í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜
    if result:
        return {
            "full_text": result,
            "topic": topic,
            "papers": all_papers
        }
    else:
        return None

def extract_keywords(query, min_length=3, max_keywords=7):
    """
    ê²€ìƒ‰ì–´ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
    cleaned_query = re.sub(r'[^\w\s]', ' ', query.lower())
    
    # ë¶ˆìš©ì–´ ëª©ë¡ (í•„ìš”ì‹œ í™•ì¥)
    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'is', 'are', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
    
    # ë‹¨ì–´ ë¶„ë¦¬ ë° ë¶ˆìš©ì–´/ì§§ì€ ë‹¨ì–´ ì œê±°
    words = [word for word in cleaned_query.split() if word not in stopwords and len(word) >= min_length]
    
    # ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜ ì œí•œ
    return words[:max_keywords]

def search_arxiv(query, max_results=5):
    """
    arXiv APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìˆ  ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ìœ ì‚¬ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ì‹ì„ ì‹œë„í•©ë‹ˆë‹¤.
    """
    try:
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords(query)
        
        # í‚¤ì›Œë“œê°€ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš° ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
        if len(keywords) < 2:
            search_query = query
        else:
            # ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ì‹ ì‹œë„
            search_queries = [
                # ì›ë³¸ ì¿¼ë¦¬
                query,
                # AND ê²€ìƒ‰ (ëª¨ë“  í‚¤ì›Œë“œ í¬í•¨)
                " AND ".join(keywords[:3]),
                # OR ê²€ìƒ‰ (ë„“ì€ ë²”ìœ„)
                " OR ".join(keywords[:3])
            ]
            
            # ë¬´ì‘ìœ„ë¡œ í•˜ë‚˜ ì„ íƒ (ë‹¤ì–‘ì„± í™•ë³´)
            import random
            search_query = random.choice(search_queries)
        
        # arXiv API ìš”ì²­ URL
        url = f"http://export.arxiv.org/api/query?search_query=all:{search_query}&start=0&max_results={max_results}"
        
        # API ìš”ì²­
        response = requests.get(url)
        
        if response.status_code == 200:
            # XML íŒŒì‹±
            # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            # XML íŒŒì‹±
            root = ET.fromstring(response.content)
            
            # ê²°ê³¼ ì¶”ì¶œ
            results = []
            for entry in root.findall('.//atom:entry', namespaces):
                title_elem = entry.find('atom:title', namespaces)
                title = title_elem.text.strip() if title_elem is not None else "ì œëª© ì—†ìŒ"
                
                summary_elem = entry.find('atom:summary', namespaces)
                summary = summary_elem.text.strip() if summary_elem is not None else "ìš”ì•½ ì—†ìŒ"
                
                published_elem = entry.find('atom:published', namespaces)
                published = published_elem.text[:10] if published_elem is not None else "ë‚ ì§œ ì—†ìŒ"
                
                # ì €ì ì¶”ì¶œ
                authors = []
                for author in entry.findall('.//atom:author/atom:name', namespaces):
                    authors.append(author.text)
                
                # PDF ë§í¬ ì¶”ì¶œ
                pdf_url = None
                for link in entry.findall('atom:link', namespaces):
                    if link.get('title') == 'pdf':
                        pdf_url = link.get('href')
                        break
                
                if pdf_url is None:
                    # ëŒ€ì²´ ë§í¬ ê²€ìƒ‰
                    for link in entry.findall('atom:link', namespaces):
                        if link.get('rel') == 'alternate':
                            pdf_url = link.get('href')
                            break
                
                # ê²°ê³¼ ì¶”ê°€
                results.append({
                    'title': title,
                    'authors': ', '.join(authors),
                    'summary': summary[:300] + "..." if len(summary) > 300 else summary,
                    'published': published,
                    'url': pdf_url,
                    'source': 'arXiv'
                })
            
            return results
        else:
            st.error(f"arXiv API ì˜¤ë¥˜: {response.status_code}")
            return []
    
    except Exception as e:
        st.error(f"arXiv ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return []

def search_crossref(query, max_results=5):
    """
    Crossref APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìˆ  ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    try:
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords(query)
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ì¤€ë¹„
        if len(keywords) < 2:
            search_query = query
        else:
            search_query = " ".join(keywords[:3])
        
        # API ìš”ì²­ URL
        email = getattr(config, 'CROSSREF_EMAIL', 'example@example.com')
        url = f"https://api.crossref.org/works?query={search_query}&rows={max_results}&mailto={email}"
        
        # API ìš”ì²­
        response = requests.get(url)
        
        if response.status_code == 200:
            data = response.json()
            
            results = []
            if 'message' in data and 'items' in data['message']:
                for item in data['message']['items']:
                    # ì œëª© ì¶”ì¶œ
                    title = "ì œëª© ì—†ìŒ"
                    if 'title' in item and item['title']:
                        title = item['title'][0]
                    
                    # ì €ì ì¶”ì¶œ
                    authors = []
                    if 'author' in item:
                        for author in item['author']:
                            name_parts = []
                            if 'given' in author:
                                name_parts.append(author['given'])
                            if 'family' in author:
                                name_parts.append(author['family'])
                            if name_parts:
                                authors.append(' '.join(name_parts))
                    
                    # ë°œí–‰ì¼ ì¶”ì¶œ
                    published = "ë‚ ì§œ ì—†ìŒ"
                    if 'published-print' in item and 'date-parts' in item['published-print']:
                        date_parts = item['published-print']['date-parts'][0]
                        if len(date_parts) >= 1:
                            published = str(date_parts[0])
                    
                    # URL ì¶”ì¶œ
                    url = None
                    if 'URL' in item:
                        url = item['URL']
                    
                    # ìš”ì•½ (ì—†ëŠ” ê²½ìš°ê°€ ë§ìŒ)
                    summary = "ìš”ì•½ ì •ë³´ ì—†ìŒ"
                    if 'abstract' in item:
                        summary = item['abstract']
                    
                    # ê²°ê³¼ ì¶”ê°€
                    results.append({
                        'title': title,
                        'authors': ', '.join(authors),
                        'summary': summary[:300] + "..." if len(summary) > 300 else summary,
                        'published': published,
                        'url': url,
                        'source': 'Crossref'
                    })
            
            return results
        else:
            st.error(f"Crossref API ì˜¤ë¥˜: {response.status_code}")
            return []
    
    except Exception as e:
        st.error(f"Crossref ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return []

def merge_search_results(arxiv_results, crossref_results, max_total=10):
    """
    ì—¬ëŸ¬ APIì—ì„œ ê°€ì ¸ì˜¨ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.
    """
    # ê²°ê³¼ ë³‘í•©
    all_results = arxiv_results + crossref_results
    
    # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
    unique_results = []
    seen_titles = set()
    
    for result in all_results:
        title_lower = result['title'].lower()
        # ì§§ì€ ì œëª© ì œì™¸ (ë„ˆë¬´ ì¼ë°˜ì ì¸ ì œëª©ì¼ ê°€ëŠ¥ì„±)
        if len(title_lower) < 10:
            continue
            
        if title_lower not in seen_titles:
            seen_titles.add(title_lower)
            
            # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            score = 0
            
            # íŠ¹ë³„íˆ ì›ë³¸ ì¿¼ë¦¬ì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = extract_keywords(result['title'])
            
            # ì œëª©ì— í‚¤ì›Œë“œê°€ ë§ì„ìˆ˜ë¡ ì ìˆ˜ ì¦ê°€
            for keyword in keywords:
                if keyword in title_lower:
                    score += 2
            
            # ìš”ì•½ì´ ìˆìœ¼ë©´ ì ìˆ˜ ì¦ê°€
            if result['summary'] and result['summary'] != "ìš”ì•½ ì •ë³´ ì—†ìŒ":
                score += 1
            
            # ì €ì ì •ë³´ê°€ ìˆìœ¼ë©´ ì ìˆ˜ ì¦ê°€
            if result['authors'] and result['authors'] != "":
                score += 1
            
            # URLì´ ìˆìœ¼ë©´ ì ìˆ˜ ì¦ê°€
            if result['url'] and result['url'] != "":
                score += 1
            
            # ì ìˆ˜ ì €ì¥
            result['relevance_score'] = score
            
            unique_results.append(result)
    
    # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
    sorted_results = sorted(unique_results, key=lambda x: x.get('relevance_score', 0), reverse=True)
    
    # ìµœëŒ€ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
    return sorted_results[:max_total]

def generate_similar_topics(topic, count=5):
    """
    ì…ë ¥ëœ ì£¼ì œì™€ ìœ ì‚¬í•œ ì—°êµ¬ ì£¼ì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ì¶”ê°€ë¡œ ì‹¤ì œ í•™ìˆ  ê²€ìƒ‰ ê²°ê³¼ë„ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.
    """
    # ì™¸ë¶€ APIë¥¼ í†µí•œ ì‹¤ì œ ì—°êµ¬ ê²€ìƒ‰
    with st.spinner("í•™ìˆ  ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì—°êµ¬ë¥¼ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤..."):
        try:
            # arXiv ê²€ìƒ‰
            arxiv_results = search_arxiv(topic, max_results=5)
            
            # Crossref ê²€ìƒ‰
            crossref_results = search_crossref(topic, max_results=5)
            
            # ê²°ê³¼ ë³‘í•©
            api_results = merge_search_results(arxiv_results, crossref_results, max_total=8)
        except Exception as e:
            st.error(f"í•™ìˆ  API ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            api_results = []
    
    # ê²€ìƒ‰ëœ ë…¼ë¬¸ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
    paper_info = ""
    if api_results:
        paper_info = "ë‹¤ìŒì€ í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ì‹¤ì œ ë…¼ë¬¸ ì •ë³´ì…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ìœ ì‚¬ ì£¼ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n"
        for i, paper in enumerate(api_results, 1):
            paper_info += f"{i}. ì œëª©: {paper['title']}\n"
            paper_info += f"   ì €ì: {paper['authors']}\n"
            paper_info += f"   ë°œí–‰: {paper['published']}\n"
            paper_info += f"   ì¶œì²˜: {paper['source']}\n"
            if paper['summary'] and paper['summary'] != "ìš”ì•½ ì •ë³´ ì—†ìŒ":
                paper_info += f"   ìš”ì•½: {paper['summary'][:150]}...\n"
            paper_info += "\n"
    
    # GPTë¥¼ í†µí•œ ìœ ì‚¬ ì£¼ì œ ìƒì„±
    prompt = f"""
    ë‹¤ìŒ ì—°êµ¬ ì£¼ì œì™€ ê´€ë ¨ëœ ìœ ì‚¬í•˜ì§€ë§Œ ë…ì°½ì ì¸ ì—°êµ¬ ì£¼ì œ {count}ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”: "{topic}"
    
    {paper_info}
    
    ê° ì£¼ì œëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:
    
    ## ì£¼ì œ 1: [ì£¼ì œëª…]
    **ì„¤ëª…**: [ì£¼ì œì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª… ë° ì—°êµ¬ ê°€ì¹˜]
    **ê´€ë ¨ ë…¼ë¬¸**: [ìœ„ ëª©ë¡ì—ì„œ ê´€ë ¨ ìˆëŠ” ë…¼ë¬¸ ì°¸ì¡°]
    
    ## ì£¼ì œ 2: [ì£¼ì œëª…]
    ...
    
    ì‹¤ì œ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜, ìƒˆë¡­ê³  ë…ì°½ì ì¸ ì—°êµ¬ ì£¼ì œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.
    ê° ì£¼ì œëŠ” ì‹¤í–‰ ê°€ëŠ¥í•˜ê³ , ëª…í™•í•˜ë©°, êµ¬ì²´ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    """
    
    with st.spinner("ìœ ì‚¬ ì£¼ì œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        ai_result = get_completion(prompt)
    
    # ìµœì¢… ê²°ê³¼ ë°˜í™˜
    return {
        "ai_generated": ai_result,
        "api_results": api_results
    }

def generate_paper_structure(topic):
    """
    ì„ íƒëœ ì£¼ì œì— ëŒ€í•œ ë…¼ë¬¸ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ë¨¼ì € ì‹¤ì œ ë…¼ë¬¸ ê²€ìƒ‰
    arxiv_papers = search_arxiv(topic, max_results=3)
    crossref_papers = search_crossref(topic, max_results=3)
    all_papers = merge_search_results(arxiv_papers, crossref_papers, max_total=5)
    
    # ê²€ìƒ‰ëœ ë…¼ë¬¸ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    paper_info = ""
    if all_papers:
        paper_info = "ë‹¤ìŒì€ í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ì‹¤ì œ ë…¼ë¬¸ ì •ë³´ì…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ë…¼ë¬¸ êµ¬ì¡°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:\n\n"
        for i, paper in enumerate(all_papers, 1):
            paper_info += f"{i}. ì œëª©: {paper['title']}\n"
            paper_info += f"   ì €ì: {paper['authors']}\n"
            paper_info += f"   ë°œí–‰: {paper['published']}\n"
            paper_info += f"   ì¶œì²˜: {paper['source']}\n"
            if paper['summary'] and paper['summary'] != "ìš”ì•½ ì •ë³´ ì—†ìŒ":
                paper_info += f"   ìš”ì•½: {paper['summary']}\n"
            paper_info += "\n"
    
    prompt = f"""
    ë‹¤ìŒ ì—°êµ¬ ì£¼ì œì— ëŒ€í•œ í•™ìˆ  ë…¼ë¬¸ êµ¬ì¡°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”: "{topic}"
    
    {paper_info}
    
    ë…¼ë¬¸ì€ ë‹¤ìŒ ì„¹ì…˜ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
    
    # [ë…¼ë¬¸ ì œëª©]
    
    ## ì´ˆë¡
    [ì—°êµ¬ì˜ ëª©ì , ë°©ë²•, ê²°ê³¼, ì˜ì˜ë¥¼ ìš”ì•½ (200-250ë‹¨ì–´)]
    
    ## 1. ì„œë¡ 
    ### 1.1 ì—°êµ¬ ë°°ê²½
    ### 1.2 ì—°êµ¬ ëª©ì  ë° ì§ˆë¬¸
    ### 1.3 ì—°êµ¬ì˜ ì¤‘ìš”ì„±
    
    ## 2. ì„ í–‰ ì—°êµ¬ ê²€í† 
    ### 2.1 ì´ë¡ ì  ë°°ê²½
    ### 2.2 ê´€ë ¨ ì—°êµ¬ ë™í–¥
    ### 2.3 ì—°êµ¬ ê³µë°± ë° ë³¸ ì—°êµ¬ì˜ ìœ„ì¹˜
    
    ## 3. ì—°êµ¬ ë°©ë²•
    ### 3.1 ì—°êµ¬ ì„¤ê³„
    ### 3.2 ë°ì´í„° ìˆ˜ì§‘ ë°©ë²•
    ### 3.3 ë¶„ì„ ë°©ë²•
    
    ## 4. ì˜ˆìƒ ê²°ê³¼
    ### 4.1 ì£¼ìš” ë°œê²¬
    ### 4.2 ê²°ê³¼ í•´ì„
    
    ## 5. ê²°ë¡  ë° ë…¼ì˜
    ### 5.1 ì—°êµ¬ ìš”ì•½
    ### 5.2 ì—°êµ¬ì˜ ì˜ì˜
    ### 5.3 í•œê³„ì  ë° í–¥í›„ ì—°êµ¬ ë°©í–¥
    
    ## ì°¸ê³ ë¬¸í—Œ
    [ì‹¤ì œ ë…¼ë¬¸ì„ ì •í™•í•œ ì¸ìš© í˜•ì‹ìœ¼ë¡œ ë‚˜ì—´]
    
    ê° ì„¹ì…˜ì— êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ì‹¤ì œ ë…¼ë¬¸ì²˜ëŸ¼ í•™ìˆ ì ì´ê³  ì²´ê³„ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    ì°¸ê³ ë¬¸í—Œì€ ì œê³µëœ ì‹¤ì œ ë…¼ë¬¸ì„ í¬í•¨í•˜ì—¬ ì •í™•í•œ ì¸ìš© í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    """
    
    with st.spinner("ë…¼ë¬¸ êµ¬ì¡°ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... (ì•½ 1ë¶„ ì†Œìš”)"):
        result = get_completion(prompt, max_tokens=2500)
    
    if result:
        return {
            "content": result,
            "papers": all_papers
        }
    else:
        return None

def generate_niche_topics(topic, count=4):
    """
    ì„ íƒëœ ì£¼ì œì™€ ê´€ë ¨ëœ í‹ˆìƒˆ ì—°êµ¬ ì£¼ì œë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
    """
    # ë¨¼ì € ì‹¤ì œ ë…¼ë¬¸ ê²€ìƒ‰
    arxiv_papers = search_arxiv(topic, max_results=3)
    crossref_papers = search_crossref(topic, max_results=3)
    all_papers = merge_search_results(arxiv_papers, crossref_papers, max_total=5)
    
    # ê²€ìƒ‰ëœ ë…¼ë¬¸ ì •ë³´ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    paper_info = ""
    if all_papers:
        paper_info = "ë‹¤ìŒì€ í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ì‹¤ì œ ë…¼ë¬¸ ì •ë³´ì…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•˜ì—¬ í‹ˆìƒˆ ì£¼ì œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”:\n\n"
        for i, paper in enumerate(all_papers, 1):
            paper_info += f"{i}. ì œëª©: {paper['title']}\n"
            paper_info += f"   ì €ì: {paper['authors']}\n"
            paper_info += f"   ë°œí–‰: {paper['published']}\n"
            paper_info += f"   ì¶œì²˜: {paper['source']}\n"
            if paper['summary'] and paper['summary'] != "ìš”ì•½ ì •ë³´ ì—†ìŒ":
                paper_info += f"   ìš”ì•½: {paper['summary'][:150]}...\n"
            paper_info += "\n"
    
    prompt = f"""
    ë‹¤ìŒ ì—°êµ¬ ì£¼ì œì™€ ê´€ë ¨ëœ í‹ˆìƒˆ ì—°êµ¬ ì£¼ì œ {count}ê°œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”: "{topic}"
    
    {paper_info}
    
    í‹ˆìƒˆ ì£¼ì œë€ ì•„ì§ ì¶©ë¶„íˆ ì—°êµ¬ë˜ì§€ ì•Šì•˜ì§€ë§Œ ì ì¬ì ìœ¼ë¡œ ê°€ì¹˜ ìˆëŠ” ì—°êµ¬ ì˜ì—­ì…ë‹ˆë‹¤.
    
    ê° í‹ˆìƒˆ ì£¼ì œëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”:
    
    ## í‹ˆìƒˆ ì£¼ì œ 1: [ì£¼ì œëª…]
    
    **ë°°ê²½**: [ì´ ë¶„ì•¼ì—ì„œ í˜„ì¬ê¹Œì§€ì˜ ì—°êµ¬ ìƒí™©]
    
    **í‹ˆìƒˆ ì˜ì—­ìœ¼ë¡œ ê³ ë ¤ë˜ëŠ” ì´ìœ **: 
    [ì™œ ì´ ì£¼ì œê°€ ì¶©ë¶„íˆ ì—°êµ¬ë˜ì§€ ì•Šì•˜ëŠ”ì§€, ì–´ë–¤ ì¸¡ë©´ì´ ê°„ê³¼ë˜ê³  ìˆëŠ”ì§€]
    
    **ì—°êµ¬ ê°€ì¹˜ì™€ ì˜í–¥ë ¥**: 
    [ì´ ì£¼ì œ ì—°êµ¬ê°€ í•™ë¬¸ì /ì‹¤ìš©ì ìœ¼ë¡œ ì–´ë–¤ ê°€ì¹˜ê°€ ìˆëŠ”ì§€]
    
    **ì œì•ˆ ì—°êµ¬ ë°©ë²•**: 
    [ì–´ë–¤ ë°©ë²•ë¡ ê³¼ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ì—°êµ¬í•  ìˆ˜ ìˆëŠ”ì§€]
    
    **ê´€ë ¨ ë…¼ë¬¸**: 
    [ìœ„ ëª©ë¡ì—ì„œ ê´€ë ¨ ìˆëŠ” ë…¼ë¬¸ ì°¸ì¡°]
    
    ì‹¤ì œ ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜, ìƒˆë¡­ê³  í˜ì‹ ì ì¸ ì—°êµ¬ í‹ˆìƒˆë¥¼ ì°¾ì•„ë‚´ì£¼ì„¸ìš”.
    ê° í‹ˆìƒˆ ì£¼ì œëŠ” ì‹¤í–‰ ê°€ëŠ¥í•˜ê³ , êµ¬ì²´ì ì´ë©°, í•™ìˆ ì  ê°€ì¹˜ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    """
    
    with st.spinner("í‹ˆìƒˆ ì£¼ì œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        result = get_completion(prompt)
    
    if result:
        return {
            "content": result,
            "papers": all_papers
        }
    else:
        return None
